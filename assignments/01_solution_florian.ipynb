{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NTDS assignment 1: Student Solution\n",
    "Florian Benedikt Roth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective of Exercise\n",
    "The aim of this exercise is to learn how to create your own, real network using data collected from the Internet and then to discover some properties of the collected network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "You might want to have a look at the following resources before starting:\n",
    "\n",
    "* [Twitter REST API](https://dev.twitter.com/rest/public)\n",
    "* [Tweepy Documentation](http://tweepy.readthedocs.io/en/v3.5.0/)\n",
    "* [Tutorial \"Mining Twitter data with Python\"](https://marcobonzanini.com/2015/03/02/mining-twitter-data-with-python-part-1/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Collect a Twitter Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to collect data from Twitter you will need to generate access tokens.  To do this you will need to register a [client application with Twitter](https://apps.twitter.com/). Once you are done you should have your tokens. You can now create a `credentials.ini` file as follows:\n",
    "```\n",
    "[twitter]\n",
    "consumer_key = YOUR-CONSUMER-KEY\n",
    "consumer_secret = YOUR-CONSUMER-SECRET\n",
    "access_token = YOUR-ACCESS-TOKEN\n",
    "access_secret = YOUR-ACCESS-SECRET\n",
    "```\n",
    "In this way you will have this information readily available to you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import random\n",
    "import configparser\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "import pickle \n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "import tweepy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the confidential token.\n",
    "credentials = configparser.ConfigParser()\n",
    "credentials.read(os.path.join('..','Data', 'credentials.ini'))\n",
    "\n",
    "#authentication\n",
    "auth = tweepy.OAuthHandler(credentials.get('twitter', 'consumer_key'), credentials.get('twitter', 'consumer_secret'))\n",
    "auth.set_access_token(credentials.get('twitter', 'access_token'), credentials.get('twitter', 'access_secret'))\n",
    "\n",
    "#construct API instance\n",
    "#deal with rate limits and notify when delayed because of rate limits\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are all set up to start collecting data from Twitter! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will construct a network with the following logic:\n",
    "\n",
    "1) We will chose a `user_id` in Twitter to be our first node. \n",
    "\n",
    "2) We will find (some) of the users who are both following `user_id` and are being followed by `user_id`. From now on we will call such users \"connections\" of `user_id`. We will place these user ids in a list called `first_nodes`. \n",
    "\n",
    "3) For every node in the list `first_nodes` we will then find (some) of the users who are following and are being followed by this node (aka the connections of this node). The user ids collected in this step will be placed in a list called `second_nodes`.\n",
    "\n",
    "4) The collection of the ids of all nodes (aka Twitter users) that we have collected so far will be placed in a list called `all_nodes`.\n",
    "\n",
    "5) Since we have only collected a subset of all possible \"connections\" for our nodes we have to check if there are any remaining inner connections that we have missed.\n",
    "\n",
    "The entire network is to be organized in a dictionary with entries that will have as key the Twitter id of the user (this is a number characterizing each user in Twitter) and as value the list of ids of his connections.\n",
    "\n",
    "So, let us begin. The first thing that you will have to do is to chose the node from which everything will start. I have chosen the Twitter account of [Applied Machine Learning Days](https://www.appliedmldays.org) that will take place in January 2018 in EPFL. You may change that if you wish to, but please make sure that the user you chose has both followers and friends and that he allows you to access this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 'RohdeSchwarz'                       #'appliedmldays' #'tudresden_de' #'barkhausensarmy' \n",
    "user_obj = api.get_user(user)               #'RohdeSchwarz'  #'dl_weekly'\n",
    "user_id =  user_obj.id\n",
    "\n",
    "print('The chosen user {} has {} followers and {} friends'.format(user_obj.screen_name, user_obj.followers_count, user_obj.friends_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell write a function that takes as an argument the Twitter id of a user and returns a list with the **ids** of his connections. Take into account the case where a user does not allow you to access this information.\n",
    "\n",
    "**Reminder:** By connections we mean users that are both followers and friends of a given user. Friend means, that the user is a follower of the given account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_connections(user_id, limit_min=5):\n",
    "    followers = []\n",
    "    friends=[]\n",
    "    connections = []\n",
    "    #limit_min = 5   # limit in minutes per node that the programm will wait additionally to get all friends and followers\n",
    "                     # if limit would be reached, the user will be replaced\n",
    "                     # take into account that this will decrease the probability of users with many connections\n",
    "                     # set to -1 to wait as long as it takes\n",
    "                     # 5000 follower/friends ~ 1 minute\n",
    "    \n",
    "    user_obj = api.get_user(user_id)\n",
    "    name ,fol_cnt, fri_cnt = user_obj.screen_name, user_obj.followers_count, user_obj.friends_count\n",
    "    \n",
    "    # ask for number of followers & friends so that requests, that would take too long are filtered\n",
    "    if max(fol_cnt, fri_cnt) > 5000:\n",
    "        minutes = np.ceil(max(fol_cnt,fri_cnt)/5000-1)\n",
    "        if limit_min < 0:\n",
    "            print('# Because {}/{} has {} followers and {} friends the time waiting for \\n  the rate limit to reset will increase by ~ {} minutes.'.format(name,user_id,fol_cnt,fri_cnt,minutes))\n",
    "        elif minutes > limit_min:\n",
    "            print('# Because {}/{} has {} followers and {} friends the time waiting for \\n  the rate limit to reset would increase by ~ {} minutes.'.format(name,user_id,fol_cnt,fri_cnt,minutes))\n",
    "            print('  Due to the chosen limit of {} minutes per node this user will be replaced'.format(limit_min))\n",
    "            connections = [float('Nan')]\n",
    "            return connections\n",
    "        \n",
    "    # get followers_ids & friends_ids\n",
    "    try:\n",
    "        for fol in tweepy.Cursor(api.followers_ids, user_id).pages():\n",
    "            followers.extend(fol)\n",
    "        for fr in tweepy.Cursor(api.friends_ids, user_id).pages():\n",
    "            friends.extend(fr)\n",
    "            \n",
    "    # if user does not allow accessing its friends & followers -> return Nan\n",
    "    except tweepy.TweepError:\n",
    "        print('# Could not access the followers/friends of the user {}/{}'.format(name,user_id))\n",
    "        connections = [float('Nan')]\n",
    "        return connections\n",
    "    \n",
    "    # find connections as intersections between friends & followers\n",
    "    connections = list(np.intersect1d(followers,friends))\n",
    "    return connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_connections = find_connections(user_id,-1)\n",
    "if np.isnan(first_connections[0]):\n",
    "    print('Choose a new starting nod.')\n",
    "else:\n",
    "    print('{} has {} connections'.format(user, len(first_connections)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect your `first_nodes` and `second_nodes` and organize your collected nodes and their connections in the dictionary called `network`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hints:\n",
    "* Use `random.choice([1,3,4])` to randomly choose a number in `[1, 3, 4]`.\n",
    "* Use the `append` and `remove` methods to add and remove an element from a Python list.\n",
    "* The `pop` method removes the last item in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_time(level, how_many):\n",
    "    # This function calculates how long the collecting data part will last under the following assumptions:\n",
    "    # 1) all the users share their followers & friends\n",
    "    # 2) no one has more than 5000 followers or friends OR limit_min = 0 \n",
    "    #     -> would lead to multiple requests per node otherwise\n",
    "    # 3) all nodes have at least 'how_many' connections\n",
    "    # \n",
    "    # real network neglecting A1 & A2 -> takes more time\n",
    "    # real network neglecting A3      -> takes less time\n",
    "    \n",
    "    n_max = 0\n",
    "    for i in range(0,level+1):       # calculating N_max\n",
    "        n_max += how_many**(i)\n",
    "    \n",
    "    # get remaining api requests in rate limit slot and and time of reset\n",
    "    remaining = api.rate_limit_status()['resources']['friends']['/friends/ids']['remaining']\n",
    "    reset = api.rate_limit_status()['resources']['friends']['/friends/ids']['reset']\n",
    "    \n",
    "    # add the amount of needed time_slots * seconds/time_slot to time of reset\n",
    "    reset += np.floor((n_max-remaining)/15)*15*60\n",
    "    print('The network you create will have up to {} nodes.'.format(n_max)) \n",
    "    print(datetime.fromtimestamp(reset).strftime('Due to restrictions of the twitter API this takes about until %H:%M o\\'clock'))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nodes(n, collection, but=[]):\n",
    "    # This function provides n random nodes from the given collection\n",
    "    # excluding the entries in 'but'\n",
    "    nodes = []\n",
    "    i = 0\n",
    "    \n",
    "    flat = [x for sublist in but for x in sublist]          # list of lists -> list containing all elements\n",
    "    \n",
    "    if not set(collection) <= set(flat):                    # dont start if entire collection is excluded\n",
    "        pool = list(set(collection)-set(flat))              # pool to choose from\n",
    "        \n",
    "        # stop when: 1) n nodes are found, or 2) no free nodes are left\n",
    "        for i in range(0,min(n, len(pool))):\n",
    "            k = random.randint(0,len(pool)-1)       # choose a random element out of the pool\n",
    "            nodes.append(pool[k])                   # add it to the chosen nodes\n",
    "            pool.remove(pool[k])                    # and delete it from the pool\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import Tracer   # for debugging insert Tracer()()  \n",
    "\n",
    "# This functions collects 'cpn' (connections/node) connections for every node in 'nodes',\n",
    "# saves them in 'nodes_on_lvl', saves nodes with all corresponding connections in all_con,\n",
    "# and calls itself until the lowest level (0) is reached.\n",
    "def build_network(nodes, all_con, cpn, level, nodes_on_lvl = [], calling_nod = -1):\n",
    "    \n",
    "    # only called the first time in the highest level to add nodes to nodes_on_lvl \n",
    "    if len(nodes_on_lvl) < (level+1):\n",
    "        nodes_on_lvl.extend([[]]*(level+1))\n",
    "        nodes_on_lvl[level] = nodes\n",
    "    trash = []                      # collect nodes that dont allow to access their friends&followers in here\n",
    "    \n",
    "    # iteration, get connections for every node\n",
    "    for nod in nodes:\n",
    "        # get connections from the twitter api\n",
    "        connections = find_connections(nod)\n",
    "\n",
    "        # user doesnt share connections\n",
    "        if np.isnan(connections[0]):\n",
    "            if calling_nod is -1:       # 'nodes' is starting nod (user_id)\n",
    "                print('  -> Choose another starting nod!')\n",
    "            else:                       # replace the node\n",
    "                nodes_on_lvl[level].remove(nod)  # delete invalid node from structure\n",
    "                trash.append(nod)                # dont remove invalid node from 'nodes', otherwise next node will be skipped\n",
    "                \n",
    "                # get one random node, that is connected to the calling node in the level above \n",
    "                # but not already in the network\n",
    "                new_nod = get_nodes(1,all_con[calling_nod],nodes_on_lvl)                \n",
    "                \n",
    "                if len(new_nod) > 0:             # get_nodes found a new node\n",
    "                    nodes.extend(new_nod)        # adding is allowed and for loop will iterate over new_nod as well\n",
    "                    nodes_on_lvl[level].extend(new_nod)\n",
    "                    name = api.get_user(new_nod[0]).screen_name\n",
    "                    print('  level {}: user was was replaced by {}/{}'.format(level,name,new_nod[0]))\n",
    "                else:\n",
    "                    print('  level {}: user was deleted'.format(level))\n",
    "                \n",
    "        # user shares connections\n",
    "        else:\n",
    "            all_con[nod] = connections                 # node with all corresponding connections is saved in dictionary\n",
    "            if level > 0:                                              ## in every level except for the lowest:\n",
    "                nxt_nodes = get_nodes(cpn, connections, nodes_on_lvl)   # choose cpn connections as next nodes\n",
    "                sublist = copy.deepcopy(nodes_on_lvl[level-1])          # add chosen nodes to structure\n",
    "                sublist.extend(nxt_nodes)\n",
    "                nodes_on_lvl[level-1] = sublist\n",
    "                \n",
    "                # call function on the next lower level\n",
    "                build_network(nxt_nodes,all_con,cpn,level-1,nodes_on_lvl,nod)\n",
    "                \n",
    "    for element in trash:\n",
    "        nodes.remove(element)                          # remove invalid nodes AFTER iterating over all nodes\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_connections = {} # dictionary for all connections => saves api requests\n",
    "nodes_on_lvl=[]      # list of sublists containing nodes of a certain level in the network\n",
    "\n",
    "level = 2            # depth of network; in this task: level = 2\n",
    "how_many = 10        # This is the number of connections you are sampling. \n",
    "                     # Keep small (e.g.3) for development, larger later (e.g. 10)\n",
    "\n",
    "    \n",
    "# make a guess how long the collection of data will take\n",
    "calc_time(level, how_many)\n",
    "\n",
    "# this function collects and assembles the data. \n",
    "build_network([user_id], all_connections, how_many, level, nodes_on_lvl)\n",
    "\n",
    "# assign the collected data from nodes_on_lvl to the different lists of nodes\n",
    "first_nodes = nodes_on_lvl[level-1]\n",
    "second_nodes = nodes_on_lvl[level-2]\n",
    "all_nodes = [x for sublist in nodes_on_lvl for x in sublist]\n",
    "\n",
    "\n",
    "print(datetime.now().time().strftime('*** Collected all data from twitter at %H:%M o\\'clock ***'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Be careful!** You should only keep a small value for the `how_many` parameter while you are developing your code. In order to answer to the questions you should raise the value of this parameter to `how_many=10` at least. This will take a while to execute because of the API rate limit (plan your time accordingly). You should also remember to submit your jupyter notebook with the **output shown for a large value of the `how_many` parameter**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} first hop nodes'.format(len(first_nodes)))\n",
    "print('There are {} second hop nodes'.format(len(second_nodes)))\n",
    "print('There are overall {} nodes in the collected network'.format(len(all_nodes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the inner connections between your collected nodes that you might have missed because you sampled the connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now all connections and not only the inner connections are found here\n",
    "# possible connections that would miss anyways: \n",
    "# first-first, first-second, second-second, start-second\n",
    "network = {}\n",
    "for a in all_nodes:\n",
    "    # using intersection between all nodes and all connections of one node\n",
    "    network[a] = list(np.intersect1d(all_connections[a], all_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to avoid doing the time consuming collection of data multiple times for \n",
    "# the same network here is the possibility to save it in a pickle file\n",
    "save = True\n",
    "if save:\n",
    "    f = open('{}_{}_{}.p'.format(user,level,how_many),'wb')\n",
    "    pickle.dump(network,f)\n",
    "    f.close()\n",
    "    print('The created network was saved in {}_{}_{}.p'.format(user,level,how_many))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Discover some of the properties of the collected network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save time it is possible to load some collected network data from a pickle file\n",
    "load = True\n",
    "filename = 'RohdeSchwarz_2_10.p'           # startinguser_level_howmany.p\n",
    "\n",
    "# avaliable networks:        'dl_weekly_2_3.p'       'dl_weekly_2_4.p'      'dl_weekly_2_5.p'\n",
    "#                            'appliedmldays_2_2.p'   'tudresden_de_2_8.p'   'RohdeSchwarz_2_10.p'\n",
    "\n",
    "if load:\n",
    "    network = {}\n",
    "    f = open(filename,'rb')\n",
    "    network = pickle.load(f)\n",
    "    f.close()\n",
    "    all_nodes = []\n",
    "    for key in network:\n",
    "        all_nodes.append(key)         # create all_nodes with the loaded network data\n",
    "    print('The network from {} was loaded'.format(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Adjacency matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congradulations! You have now created a dictionary that describes a real Twitter network!\n",
    "We now want to transform this dictionary into the adjacency (or weight) matrix that you learned about in your first class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparation for creatign the matrix: \n",
    "# 1) empty quadratic matrix of correct size \n",
    "W = np.zeros([len(all_nodes),len(all_nodes)], dtype=int)\n",
    "# 2) dictionary with nod -> index, that will be position in matrix\n",
    "code = {}   \n",
    "for ind,k in enumerate(network):         \n",
    "    code[k] = ind\n",
    "    \n",
    "# create matrix applying the node-index transform\n",
    "for nod in network:\n",
    "    for connection in network[nod]:\n",
    "        W[code[nod]][code[connection]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that a weight matrix should be symmetric. Check if it is:  \n",
    "This code was combined with the next part, so that checking and fixing if needed are combined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:**\n",
    "It might happen that $W \\neq W^{T} $ for some $(i,j)$. Explain why this might be the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here:** Depending on the implementation one can get a non-symmetric weight matrix W, if one does not assign the connection from a to b automatically to b to a. If one does that but checks before the friends and followers of twitter user b, one can run into a problem in the case that the twitter user does not allow to enter its connections followers and friends. Another problem would occur if one finds not all connections of a user. This can happen if the function find_connections does not use the Cursor object and the amount of found friends and followers is 5000 as maximum.  \n",
    "In this implementation though:\n",
    "* twitter users, that dont allow to access their connections are replaced if possible, otherwise deleted\n",
    "* the cursor object is used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impose your weight matrix to be symmetric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if matrix is symmetric\n",
    "if len(W[np.nonzero(W-W.transpose())]) is not 0:\n",
    "    # Make W symmetric\n",
    "    bigger = W.transpose() > W       # bigger is True, where a connection in W is missing\n",
    "    W = W - W*bigger + W.transpose()*bigger   # The term 'W*bigger' is only for security, W should be zero at these points\n",
    "    print('W was not symmetric but it is now')\n",
    "else:\n",
    "    print('W is symmetric')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the weight matrix of your collected network.\n",
    "\n",
    "Hint: use `plt.spy()` to visualize a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize matrix\n",
    "plt.spy(W)\n",
    "plt.title('Adjacency Matrix W')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:**\n",
    "What is the maximum number of links $L_{max}$ in a network with $N$ nodes (where $N$ is the number of nodes in your collected network)? How many links $L$ are there in your collected network? Comment on how $L$ and $L_{max}$ compare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here:**\n",
    "* Complete network: In a complete network, every nod has a link to every other node. Therefore  \n",
    "$L_{max} = \\frac{N\\dot(N-1)}{2}$\n",
    "* For this created network we neglect the amount of connections, that were missed because of the sampling and we assume, that every node has at least how_many (in the formula $n$) nodes. Taking into account the depth or level of the network $l$, that was for this assignment $l=2$, the amount of nodes is  \n",
    "$N = 1+n+n^{2}+...+n^{l} = \\sum_{i=0}\\limits^{l}n^{i}$  \n",
    "The amount of links is  \n",
    "$L = n+n^{2}+...+n^{l} = \\sum\\limits_{i=1}^{l}n^{i}$  \n",
    "Therefore the amount of links can be expressed as $L = N-1$  \n",
    "* For $N >> 1$ the amount of links grows for a complete net work with $L_{max} \\sim N^{2}$  \n",
    "and for our specific network with $L \\approx N$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Degrees distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a histogram of the degree distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of row/column equals connections of specific user\n",
    "p = W.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(p,max(p),normed=1);     # hist does the rest of the work, normed returns probablilities\n",
    "plt.xlabel('degree')\n",
    "plt.ylabel('probablility')\n",
    "plt.title('histrogram')\n",
    "plt.xlim([1,max(p)])\n",
    "if max(p) < 10:\n",
    "    plt.xticks(np.arange(1,max(p)+1,1))               # avoid decimal values as ticks which dont make sense at a histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:** Comment on the plot. What do you observe? Would you expect a similar degree disribution in the complete Twitter network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here:**  \n",
    "In this collected dataset there are a lot of nods having only one connection. Then the amount decreases very fast, roughly with $\\frac{1}{k}$ keeping it at a low but quite constant level for higher values of $k$. The maximum degree in this network is 18.  \n",
    "I think, that the histogram of the complete Twitter network would look similar with a minor difference.  Probably the degree with the highest probability is not 1 but slightly higher. Almost every user shares a couple connections and the reason we found that many in our data set with only one connection is, that we stopped after the second_nodes and did not continue going on. Apart from that I think it looks similar with only a few users having a very big amount of connections. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Average degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the average degree of your collected network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p: degree per nod -> mean(p): average degree\n",
    "d_avg = np.mean(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Diameter of the collected network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4:** What is the diameter of the collected network? Please justify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here:** \n",
    "The maximum distance between two nodes of the network is 4, because we went only 2 layers down from the starting user_id. This makes a maximum distance of 2 hops up from the bottom layer to user_id and from there 2 hops down to another node in the base layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Pruning the collected network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice that some nodes have very few connections and hence our matrix is very sparse. Prune the collected network so that you keep only the nodes that have a degree that is greater than the average degree and plot the new adjacency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the indices of nodes that have a lower degree than average\n",
    "indices = []\n",
    "for ind,nods in enumerate(p):\n",
    "    if nods < d_avg:\n",
    "        indices.append(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the pruned matrix by deleting the rows and columns to the belonging indices\n",
    "Wpruned = np.delete(copy.copy(W),indices,0)\n",
    "Wpruned = np.delete(Wpruned,indices,1)\n",
    "\n",
    "# compare d_avg to before\n",
    "d_avg_p = np.mean(Wpruned.sum(1))\n",
    "print('By pruning the average degree of the network changed form {0:.3f} to {1:.3f}'.format(d_avg,d_avg_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.spy(Wpruned, markersize=10)\n",
    "plt.title('Adjacency Matrix W');"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 1
}
