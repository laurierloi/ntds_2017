{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NTDS assignment 4: graph signal processing\n",
    "[MichaÃ«l Defferrard](http://deff.ch), *PhD student*, [EPFL LTS2](http://lts2.epfl.ch)\n",
    "\n",
    "In this assignment we'll use the [PyGSP](https://github.com/epfl-lts2/pygsp) to do some Graph Signal Processing (GSP). This fourth assignement is shorter than the last one in order to let you focus on your projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse, spatial\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pygsp import graphs, filters, plotting\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (17, 5)\n",
    "plotting.BACKEND = 'matplotlib'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get an import error about the PyGSP, install it with `pip install pygsp` or `conda install -c conda-forge pygsp`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Data\n",
    "\n",
    "For this assignment, we'll again use the [Free Music Archive dataset](https://github.com/mdeff/fma), as for the third. This time, I'm giving you all the data. Let's load the musical genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth genres.\n",
    "tracks = pd.read_csv('../data/fma_tracks.csv', index_col=0, squeeze=True)\n",
    "\n",
    "# Complete missing genres.\n",
    "tracks[:10] = [21, 21, 27, 12, 31, 89, 36, 25, 21, 12]\n",
    "\n",
    "# Convert to top-level genres.\n",
    "tracks = tracks.apply(lambda gid: 21 if gid in [21, 83, 100, 539, 542, 811] else 12)\n",
    "assert set(tracks.unique()) == {12, 21} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Graph\n",
    "\n",
    "Given a data matrix $\\mathbf{X} = [\\mathbf{x}_1, \\ldots, \\mathbf{x}_N]^\\intercal \\in \\mathbb{R}^{N \\times d}$, where each $\\mathbf{x} \\in \\mathbb{R}^d$ of the $N=2000$ row is a sample for which we have $d$ features, we constructed in the last assigment a similarity graph defined as\n",
    "$$\\mathbf{W}[i,j] = \\exp(-d^2(\\mathbf{x}_i - \\mathbf{x}_j) / \\sigma^2).$$\n",
    "\n",
    "We construct below a PyGSP graph object with the edge weights from the last assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = sparse.load_npz('../data/fma_graph.npz')\n",
    "G = graphs.Graph(weights, gtype='FMA genres')\n",
    "del weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the PyGSP, compute the normalized graph Laplacian, defined as\n",
    "$$\\mathbf{L} = \\mathbf{I} - \\mathbf{D}^{-1/2} \\mathbf{W} \\mathbf{D}^{-1/2},$$\n",
    "where $\\mathbf{D} \\in \\mathbb{R}^{N \\times N}$ is the diagonal degree matrix and $\\mathbf{I}$ is the identity matrix.\n",
    "\n",
    "Hints:\n",
    "* Look at the documentation [here](https://pygsp.readthedocs.io/en/latest/reference/graphs.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Fourier basis\n",
    "\n",
    "The PyGSP can compute the graph Fourier basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.compute_fourier_basis(recompute=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vector of eigenvalues are then available at `G.e`, and the Fourier basis, i.e., the eigenvectors, at `G.U`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(G.e);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot, can you infer if the Fourier basis was computed from a combinatorial or normalized Laplacian?\n",
    "\n",
    "**Your answer here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Layout\n",
    "\n",
    "To visualize our graph, we need to give each node a coordinate in 2 or 3 dimensions. Embed the graph in 2D with Laplacian eigenmaps and visualize it.\n",
    "\n",
    "Hints:\n",
    "* Use `G.set_coordinates()` and `G.plot()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize signals, like the genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.plot_signal(tracks, vertex_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or the eigenvectors / Fourier modes.\n",
    "\n",
    "Hint:\n",
    "* If you did embed the graph correctly, you should see how the second and third eigenvectors are aligned with the x and y axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.plot_signal(G.U[:, 5], vertex_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Filter localization\n",
    "\n",
    "We define below the low-pass filter\n",
    "$$\\hat{g}(x) = \\exp \\left( \\frac{-\\tau x}{\\lambda_{\\text{max}}} \\right).$$\n",
    "Its frequency response is depicted.\n",
    "\n",
    "The vertical bars in the plot represents the eigenvalues of the graph. While the filter is continuous, it is only evaluated at those eigenvalues when filtering a graph signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = filters.Heat(G, tau=10)\n",
    "f.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To observe how our kernel looks like in the node domain, we would like to localize it on node $i$. Given the Fourier basis $\\mathbf{U}$, the filter kernel $\\hat{g}(\\lambda)$, and the diagonal matrix of eigenvalues $\\mathbf\\Lambda$, what is the expression of the localized kernel $T_i g \\in \\mathbb{R}^N$?\n",
    "\n",
    "**Your answer here.**\n",
    "\n",
    "Now localize it at node $i=300$ and observe the result.\n",
    "\n",
    "Hints:\n",
    "* You can evaluate the filter with `f.evaluate()`.\n",
    "* You can filter a signal with `f.filter()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NODE = 300\n",
    "\n",
    "s =  # Your code here.\n",
    "\n",
    "G.plot_signal(s, vertex_size=50, highlight=NODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, should the plotted graph signal be smooth?\n",
    "\n",
    "**Your answer here.**\n",
    "\n",
    "Confirm your intuition by looking at the signal `s` in the Fourier domain. Compare with the graph Fourier transform (GFT) of a delta signal\n",
    "$$\\delta_i[j] =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } j = i, \\\\\n",
    "0 & \\text{otherwise.}\n",
    "\\end{cases}$$\n",
    "\n",
    "Hints:\n",
    "* You can compute the GFT with `G.gft()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Transductive learning\n",
    "\n",
    "In this problem, we'll consider having the labels for some percentage of our $N=2000$, but missing those of the other half. Those of you who have done some [Machine Learning (ML)](https://en.wikipedia.org/wiki/Machine_learning) will surely recognize here a [supervised learning](https://en.wikipedia.org/wiki/Supervised_learning) problem and will say: yeah, let's train a classifier on the training data, and predict the missing labels! While they would not be wrong in doing so, they would not use the unlabeled data at all for training. The setup where test cases are known a-priori is called [transductive learning](https://en.wikipedia.org/wiki/Transduction_%28machine_learning%29). So let's exploit the structure of the data domain with similarity graph!\n",
    "\n",
    "Define $\\mathbf{y} \\in \\{-1,1\\}^n, n < N,$ as the subset of known labels (the musical genres) and $M$ as a masking matrix indicating the observations, with $\\operatorname{diag}(M) \\in \\{0,1\\}^N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth.\n",
    "x = np.ones(G.N)\n",
    "x[tracks == 21] = -1\n",
    "\n",
    "def prepare_observations(p):\n",
    "    \"\"\"Prepare observations, where p is the percentage of values to keep.\"\"\"\n",
    "    rs = np.random.RandomState(42)\n",
    "    M = np.diag(rs.uniform(size=G.N) < p)\n",
    "    return M.dot(x)\n",
    "\n",
    "# Play with the percentage of observed values.\n",
    "y = prepare_observations(p=0.1)\n",
    "plt.hist(y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem we then want to solve is\n",
    "$$\\mathbf{x}^* = \\operatorname*{arg\\,min}_{\\mathbf{x} \\in \\mathbb{R}^N} \\|\\mathbf{y} - \\mathbf{Mx}\\|_2^2 + \\alpha \\mathbf{x}^\\intercal \\mathbf{L} \\mathbf{x},$$\n",
    "where $\\alpha$ is an hyper-parameter which controls the trade-off between the data fidelity term and the smoothness prior.\n",
    "\n",
    "What is the solution of this problem?\n",
    "\n",
    "**Your answer here.**\n",
    "\n",
    "Implement it below.\n",
    "\n",
    "Hints:\n",
    "* The solution of a linear system of equations can be found with `np.linalg.solve()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve(y, alpha):\n",
    "    \"\"\"\n",
    "    Solve the optimization problem.\n",
    "    \n",
    "    Parameters:\n",
    "        y: the observations\n",
    "        alpha: the balance between fidelity and smoothness prior.\n",
    "    \n",
    "    Returns:\n",
    "        x_pred: the predicted class\n",
    "        x_star: the solution of the optimization problem\n",
    "    \"\"\"\n",
    "    \n",
    "    # Your code here.\n",
    "\n",
    "    return x_pred, x_star\n",
    "\n",
    "x_pred, x_star = solve(y, alpha=1)\n",
    "\n",
    "# Be sure that the prediction is binary.\n",
    "np.testing.assert_equal(abs(x_pred), 1)\n",
    "\n",
    "# Error rate.\n",
    "err = np.count_nonzero(x - x_pred)\n",
    "print('{} errors ({:.2%})'.format(err, err/G.N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the various graph signals:\n",
    "1. The ground truth `x`.\n",
    "1. The observations `y`.\n",
    "1. The smooth solution `x_star`.\n",
    "1. The binary prediction `x_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.plot_signal(x, vertex_size=20)\n",
    "G.plot_signal(y, vertex_size=20)\n",
    "G.plot_signal(x_star, vertex_size=20)\n",
    "G.plot_signal(x_pred, vertex_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the prediction accuracy as a function of $p \\approx \\frac{n}{N}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "\n",
    "for p in [0.1, 0.3, 0.5, 0.7, 0.9]:\n",
    "    # Your code here.\n",
    "    print('{}: {:6.2%}'.format(p, err/G.N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Conclusion\n",
    "\n",
    "In this assignment, you hopefully got some intuitions about the graph Fourier transform (GFT), and have an idea on how we can leverage graphs to exploit geometry in the data. Moreover, we saw that the [PyGSP](https://github.com/epfl-lts2/pygsp) can considerably ease Graph Signal Processing (GSP)!\n",
    "\n",
    "If you feel confident about predicting genres on the [FMA](https://github.com/mdeff/fma), consider participating to our [genre recognition challenge](https://www.crowdai.org/challenges/www-2018-challenge-learning-to-recognize-musical-genre) to label 35,000 tracks! I will surely have a semester or master project to offer to those who do well. :)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
